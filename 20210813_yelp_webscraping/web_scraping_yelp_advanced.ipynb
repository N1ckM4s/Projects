{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_scraping_yelp_advanced.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NVb9Aeey-JV"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNuI9Imsxcr7"
      },
      "source": [
        "def top_frequent(text, num_words):\n",
        "  #frequency of most common words\n",
        "  import spacy\n",
        "  from collections import Counter\n",
        "\n",
        "  nlp = spacy.load(\"en\")\n",
        "  text = text\n",
        "  \n",
        "  #lemmatization\n",
        "  doc = nlp(text)\n",
        "  token_list = list()\n",
        "  for token in doc:\n",
        "    #print(token, token.lemma_)\n",
        "    token_list.append(token.lemma_)\n",
        "  token_list\n",
        "  lemmatized = ''\n",
        "  for _ in token_list:\n",
        "    lemmatized = lemmatized + ' ' + _\n",
        "  lemmatized\n",
        "\n",
        "  #remove stopwords and punctuations\n",
        "  doc = nlp(lemmatized)\n",
        "  words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
        "  word_freq = Counter(words)\n",
        "  common_words = word_freq.most_common(num_words)\n",
        "  return common_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DW3G7Z1Nscf"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "rest_dict = [\n",
        "              {  \"name\"   : \"the-cortez-raleigh\",\n",
        "                  \"link\"  : \"https://www.yelp.com/biz/the-cortez-raleigh?osq=Restaurants&start=\",\n",
        "                  \"pages\" : 3\n",
        "              },\n",
        "              {  \"name\"   : \"rosewater-kitchen-and-bar-raleigh\",\n",
        "                  \"link\"  : \"https://www.yelp.com/biz/rosewater-kitchen-and-bar-raleigh?osq=Restaurants&start=\",\n",
        "                  \"pages\" : 3\n",
        "              }\n",
        "]\n",
        "\n",
        "def scrape(rest_list):\n",
        "  all_comment_list = list()\n",
        "  for rest in rest_list:\n",
        "    comment_list = list()\n",
        "    for pag in range(1, rest['pages']):\n",
        "      try:\n",
        "        time.sleep(5)\n",
        "\n",
        "        #URL = \"https://www.yelp.com/biz/the-cortez-raleigh?osq=Restaurants&start=\"+str(pag*10)+\"&sort_by=rating_asc\"\n",
        "        URL = rest['link']+str(pag*10)\n",
        "        print(rest['name'], 'downloading page ', pag*10)\n",
        "        page = requests.get(URL)\n",
        "\n",
        "        #next step: parsing\n",
        "        soup = BeautifulSoup(page.content, 'lxml')\n",
        "        soup\n",
        "\n",
        "        for comm in soup.find(\"yelp-react-root\").find_all(\"p\", {\"class\" : \"comment__373c0__Nsutg css-n6i4z7\"}):\n",
        "          comment_list.append(comm.find(\"span\").decode_contents())\n",
        "          print(comm.find(\"span\").decode_contents())\n",
        "      except:\n",
        "        print(\"could not work properly!\")\n",
        "    all_comment_list.append([comment_list, rest['name']])\n",
        "  return all_comment_list\n",
        "\n",
        "reviews = scrape(rest_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg3j_ENS19kl"
      },
      "source": [
        "df = pd.DataFrame(reviews).explode(0)\n",
        "df = df.reset_index(drop=True)\n",
        "df[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSa5Z_uR5PnW"
      },
      "source": [
        "def perform_sentiment(x):\n",
        "  testimonial = TextBlob(x)\n",
        "  #testimonial.sentiment (polarity, subjectvity)\n",
        "  testimonial.sentiment.polarity\n",
        "  #sentiment_list.append([sentence, testimonial.sentiment.polarity, testimonial.subjectivity])\n",
        "  return testimonial.sentiment.polarity\n",
        "\n",
        "df[2] = df[0].apply(lambda x : perform_sentiment(x))\n",
        "df = df.sort_values(2, ascending=False)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjyx4mIgyjZq"
      },
      "source": [
        "text = ' '.join(list(df[0].values[0:20]))\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc49LCzAyegm"
      },
      "source": [
        "top_frequent(text, 100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}