{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "automatic string clustering.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8F4YrqKem0K"
      },
      "source": [
        "Sources: https://stackoverflow.com/questions/66884270/text-data-clustering-with-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjh2hpopxVh3"
      },
      "source": [
        "!pip install distance\n",
        "!pip install texthero\n",
        "!pip install fuzzywuzzy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYoSraR6qenf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "import distance\n",
        "import texthero as hero\n",
        "from fuzzywuzzy import process\n",
        "from operator import itemgetter\n",
        "\n",
        "df = pd.DataFrame([\n",
        "              'Italy', \n",
        "              'Italie', \n",
        "              'Italia', \n",
        "              'Italy',\n",
        "              'Russia',\n",
        "              'Russie',\n",
        "              'Russia',\n",
        "              'Russi',\n",
        "])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGh1_Qpqxczf"
      },
      "source": [
        "#IT MAKES A HUGE DIFFERENCE IF WE HAVE OR NOT HAVE LOWERCASE\n",
        "#it uses Levenshtein distance between words\n",
        "def string_matching(list1, damping, Verbose=False):\n",
        "  #print(damping)\n",
        "  words = np.asarray(list1) #So that indexing with a list will work\n",
        "  lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
        "  affprop = AffinityPropagation(affinity=\"precomputed\", damping=damping)\n",
        "  affprop.fit(lev_similarity)\n",
        "    \n",
        "  cluster_list = list()\n",
        "  for cluster_id in np.unique(affprop.labels_):\n",
        "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
        "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
        "    cluster_str = \", \".join(cluster)\n",
        "    if Verbose == True:\n",
        "      print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
        "    cluster_list.append(exemplar)\n",
        "  #print(cluster_list)\n",
        "  return cluster_list\n",
        "\n",
        "#string_matching(try_list, 0.6)\n",
        "def extract_classes(df, col, top_n, damping, Verbose=False):\n",
        "  #try_list = hero.clean(df['ProprietaEdificio'].dropna()[2:]).unique()\n",
        "  #try_list\n",
        "  top50_clean = list(hero.clean(df[col].value_counts())[0:top_n].dropna().reset_index()['index'])\n",
        "  top50_clusters = string_matching(top50_clean, damping)\n",
        "  if Verbose == True:\n",
        "    print('top_' + str(top_n) + ' cleaned:', '\\t\\t', top50_clean)\n",
        "    print('top_' + str(top_n) + ' clusters:', '\\t', top50_clusters)\n",
        "    print('\\n')\n",
        "  return top50_clusters\n",
        "\n",
        "def clean_df(df, col, choices, score_cutoff, Verbose=False):\n",
        "  df2 = pd.DataFrame(df[col].dropna().apply(lambda x : process.extractBests(x, choices, score_cutoff=score_cutoff)))\n",
        "  df2['original'] = df[col]\n",
        "  df2 = df2[df2[col].apply(len) > 0]\n",
        "  if Verbose == True:\n",
        "    print('original:', df2[col].count())\n",
        "    print('matched:', df[col].count())\n",
        "    print('percentage:', df[col].count()/df2[col].count())\n",
        "  def select_best(tuples):\n",
        "    return max(tuples, key=itemgetter(1))[0]\n",
        "\n",
        "  df2['best_match'] = df2[0].apply(select_best)\n",
        "  df2.columns = ['matches', 'original', 'best_match']\n",
        "  return df2\n",
        "\n",
        "choices = extract_classes(df, 0, 7, 0.5, True)\n",
        "choices = ['Russia', 'Italia']\n",
        "cleaned_df = clean_df(df, 0, choices, 20, True)\n",
        "cleaned_df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}